{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on User Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Libraries Imported!\n"
     ]
    }
   ],
   "source": [
    "# for text processing and cleaning \n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string \n",
    "string.punctuation\n",
    "\n",
    "# remove warnings \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# for sentiment analysis \n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# for topic modeling using LDA \n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel \n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pyLDAvis\n",
    "# import pyLDAvis.gensim \n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('‚úîÔ∏è Libraries Imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK with Vader\n",
    "[NLTK Vader Documentation](https://www.nltk.org/howto/sentiment.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understand the Raw Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import nltk \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11648, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into dataframe, skip the error lines\n",
    "df = pd.read_csv('c:\\\\Users\\\\hs324\\\\OneDrive\\\\Desktop\\\\Class_Files\\\\06_2022Fall\\\\04_Practicum\\\\Quantilope_Core\\\\data\\\\five_brands_text.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>username</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_tweets</th>\n",
       "      <th>author_description</th>\n",
       "      <th>author_location</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87144412</td>\n",
       "      <td>GarrettKGray</td>\n",
       "      <td>376</td>\n",
       "      <td>12086</td>\n",
       "      <td>Land Economist &amp; Economic Development Speciali...</td>\n",
       "      <td>Coos Bay, OR</td>\n",
       "      <td>@ShaneDaleAZ Totally. The Nike uniforms since ...</td>\n",
       "      <td>2022-09-30 23:46:59+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>492330913</td>\n",
       "      <td>LockDown_Lopes</td>\n",
       "      <td>470</td>\n",
       "      <td>97876</td>\n",
       "      <td>@nicekicks, sports, &amp; memes | University of Ar...</td>\n",
       "      <td>Scottsdale, AZ</td>\n",
       "      <td>Hats off to Tom Sachs and the marketing team a...</td>\n",
       "      <td>2022-09-30 23:43:45+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37706001</td>\n",
       "      <td>RyanGensler</td>\n",
       "      <td>6683</td>\n",
       "      <td>13623</td>\n",
       "      <td>315 Born and Raised: Assistant Basketball Coac...</td>\n",
       "      <td>Champaign, IL</td>\n",
       "      <td>The look on @makiracook face! üòÇ \\n\\nThanks @Ni...</td>\n",
       "      <td>2022-09-30 23:38:15+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17417435</td>\n",
       "      <td>ShellzBoss</td>\n",
       "      <td>564</td>\n",
       "      <td>22093</td>\n",
       "      <td>#TeamLibra #TeamLesbian Hibernating, should be...</td>\n",
       "      <td>Maryland, Michigan</td>\n",
       "      <td>Check out my new pickup from Nike‚Å† SNKRS: http...</td>\n",
       "      <td>2022-09-30 23:35:28+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>853714067692806144</td>\n",
       "      <td>DJKingJam</td>\n",
       "      <td>395</td>\n",
       "      <td>3774</td>\n",
       "      <td>Jordan Shoe collector || DJ Jamez || Music Pro...</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>@jameslfreelance @Jumpman23 @Nike @nikestore O...</td>\n",
       "      <td>2022-09-30 23:15:57+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id        username  author_followers  author_tweets  \\\n",
       "0            87144412    GarrettKGray               376          12086   \n",
       "1           492330913  LockDown_Lopes               470          97876   \n",
       "2            37706001     RyanGensler              6683          13623   \n",
       "3            17417435      ShellzBoss               564          22093   \n",
       "4  853714067692806144       DJKingJam               395           3774   \n",
       "\n",
       "                                  author_description      author_location  \\\n",
       "0  Land Economist & Economic Development Speciali...         Coos Bay, OR   \n",
       "1  @nicekicks, sports, & memes | University of Ar...       Scottsdale, AZ   \n",
       "2  315 Born and Raised: Assistant Basketball Coac...        Champaign, IL   \n",
       "3  #TeamLibra #TeamLesbian Hibernating, should be...  Maryland, Michigan    \n",
       "4  Jordan Shoe collector || DJ Jamez || Music Pro...          Seattle, WA   \n",
       "\n",
       "                                                text  \\\n",
       "0  @ShaneDaleAZ Totally. The Nike uniforms since ...   \n",
       "1  Hats off to Tom Sachs and the marketing team a...   \n",
       "2  The look on @makiracook face! üòÇ \\n\\nThanks @Ni...   \n",
       "3  Check out my new pickup from Nike‚Å† SNKRS: http...   \n",
       "4  @jameslfreelance @Jumpman23 @Nike @nikestore O...   \n",
       "\n",
       "                  created_at  retweets  replies  likes  quote_count  \n",
       "0  2022-09-30 23:46:59+00:00         0        0      0            0  \n",
       "1  2022-09-30 23:43:45+00:00         0        0      0            0  \n",
       "2  2022-09-30 23:38:15+00:00         1        1     27            0  \n",
       "3  2022-09-30 23:35:28+00:00         0        1      0            0  \n",
       "4  2022-09-30 23:15:57+00:00         0        0      2            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @ShaneDaleAZ Totally. The Nike uniforms since ...\n",
       "1    Hats off to Tom Sachs and the marketing team a...\n",
       "2    The look on @makiracook face! üòÇ \\n\\nThanks @Ni...\n",
       "3    Check out my new pickup from Nike‚Å† SNKRS: http...\n",
       "4    @jameslfreelance @Jumpman23 @Nike @nikestore O...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_raw = df['text']\n",
    "text_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Text Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Transform emojis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for emoji handling \n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hilarious face_with_tears_of_joy. The feeling of making a sale smiling_face_with_sunglasses, The feeling of actually fulfilling orders unamused_face'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for converting emojis into word\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "text1 = \"Hilarious üòÇ. The feeling of making a sale üòé, The feeling of actually fulfilling orders üòí\"\n",
    "convert_emojis(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Happy_face_smiley Happy_face_smiley'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for converting emoticons into word\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = text.replace(emot, EMOTICONS_EMO[emot].replace(\" \",\"_\"))\n",
    "    return text\n",
    "\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As __*lemmatization*__ will return the words to its original form based on its semantic meaning, while __*stemming*__ basically will chop off the suffix and sometimes loose information. <br><br>\n",
    "Although that stemming being a rule-based approach, it runs faster than lemmatization, considering our corpuses size isn't too big and the more accurate nature of the canonical dictionary-based approach of lemma, we'll use lemmatization in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_lemma_show(var):\n",
    "    \"\"\"\n",
    "    Function for text preprocessing with Lemmatizing with POS tag.\n",
    "    \"\"\"\n",
    "    # remove the stop words \n",
    "    sw = set(stopwords.words('english'))\n",
    "    my_text = [word for word in str(var).split() if word not in sw]\n",
    "    my_text = \" \".join(my_text)\n",
    "    print(f'after removing stop words: {my_text}')\n",
    "\n",
    "    # lowercase \n",
    "    my_text = my_text.lower()\n",
    "    print(f'after lower case: {my_text}')\n",
    "\n",
    "    # removal of URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    my_text = url_pattern.sub(r'', my_text)\n",
    "    print(f'after removing urls: {my_text}')\n",
    "\n",
    "    # removal of HTML Tags\n",
    "    my_text = BeautifulSoup(my_text, \"lxml\").text\n",
    "    print(f'after removing HTMLs: {my_text}')\n",
    "\n",
    "    # tokenize the word using nltk  \n",
    "    my_text = nltk.word_tokenize(my_text)\n",
    "    print(f'after tokenize: {my_text}')\n",
    "\n",
    "    # lemmatizing and using grouped word chuncks \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\": wordnet.NOUN,\"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    # lemmatizing \n",
    "    pos_tagged_text = nltk.pos_tag(my_text) \n",
    "    my_text = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "    print(f'after lemma: {my_text}')\n",
    "    \n",
    "    # change emojis into words \n",
    "    for emot in UNICODE_EMOJI:\n",
    "        my_text = my_text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    print(f'after changing emojis: {my_text}')\n",
    "\n",
    "    # remove not english characters, lower case and split the text \n",
    "    my_text = re.sub('[^A-Za-z0-9]+', \" \", my_text).lower().strip() \n",
    "    print(f'after removing mentions: {my_text}')\n",
    "\n",
    "    # convert the text to list as the vectorized words  \n",
    "    ## my_text = my_text.split(\" \")\n",
    "\n",
    "    return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    Whatcha think of these? #Leggings #scrunchy #F...\n",
       "11    Sheesh. These Dunks go crazy @nike #justdoit h...\n",
       "12    I'm at Nike Soho - @nikenyc in New York https:...\n",
       "13    NEW KICKS üëÄ‚ô•Ô∏èüòçü´†üí∏\\n#Nike #AF1 #airforce1 @ Orla...\n",
       "14    ‚ÄúGood Times‚Äù last night at City Parks Foundati...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try the data in small bulk \n",
    "test = text_raw[10:15]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing stop words: Whatcha think these? #Leggings #scrunchy #FridayVibes #Leggingsass #onlyfansgirl #onlyfans #nike #kicks #fire #AllNaturalBeauty #thick #smile #workmode #BREAKING #amazon #wishlist #AmazonWishList https://t.co/7OzpZ4jdEU\n",
      "after lower case: whatcha think these? #leggings #scrunchy #fridayvibes #leggingsass #onlyfansgirl #onlyfans #nike #kicks #fire #allnaturalbeauty #thick #smile #workmode #breaking #amazon #wishlist #amazonwishlist https://t.co/7ozpz4jdeu\n",
      "after removing urls: whatcha think these? #leggings #scrunchy #fridayvibes #leggingsass #onlyfansgirl #onlyfans #nike #kicks #fire #allnaturalbeauty #thick #smile #workmode #breaking #amazon #wishlist #amazonwishlist \n",
      "after removing HTMLs: whatcha think these? #leggings #scrunchy #fridayvibes #leggingsass #onlyfansgirl #onlyfans #nike #kicks #fire #allnaturalbeauty #thick #smile #workmode #breaking #amazon #wishlist #amazonwishlist \n",
      "after tokenize: ['whatcha', 'think', 'these', '?', '#', 'leggings', '#', 'scrunchy', '#', 'fridayvibes', '#', 'leggingsass', '#', 'onlyfansgirl', '#', 'onlyfans', '#', 'nike', '#', 'kicks', '#', 'fire', '#', 'allnaturalbeauty', '#', 'thick', '#', 'smile', '#', 'workmode', '#', 'breaking', '#', 'amazon', '#', 'wishlist', '#', 'amazonwishlist']\n",
      "after lemma: whatcha think these ? # legging # scrunchy # fridayvibes # leggingsass # onlyfansgirl # onlyfans # nike # kick # fire # allnaturalbeauty # thick # smile # workmode # break # amazon # wishlist # amazonwishlist\n",
      "after changing emojis: whatcha think these ? # legging # scrunchy # fridayvibes # leggingsass # onlyfansgirl # onlyfans # nike # kick # fire # allnaturalbeauty # thick # smile # workmode # break # amazon # wishlist # amazonwishlist\n",
      "after removing mentions: whatcha think these legging scrunchy fridayvibes leggingsass onlyfansgirl onlyfans nike kick fire allnaturalbeauty thick smile workmode break amazon wishlist amazonwishlist\n",
      "after removing stop words: Sheesh. These Dunks go crazy @nike #justdoit https://t.co/B3mGmgy69M\n",
      "after lower case: sheesh. these dunks go crazy @nike #justdoit https://t.co/b3mgmgy69m\n",
      "after removing urls: sheesh. these dunks go crazy @nike #justdoit \n",
      "after removing HTMLs: sheesh. these dunks go crazy @nike #justdoit \n",
      "after tokenize: ['sheesh', '.', 'these', 'dunks', 'go', 'crazy', '@', 'nike', '#', 'justdoit']\n",
      "after lemma: sheesh . these dunk go crazy @ nike # justdoit\n",
      "after changing emojis: sheesh . these dunk go crazy @ nike # justdoit\n",
      "after removing mentions: sheesh these dunk go crazy nike justdoit\n",
      "after removing stop words: I'm Nike Soho - @nikenyc New York https://t.co/JU0CtiBxxI\n",
      "after lower case: i'm nike soho - @nikenyc new york https://t.co/ju0ctibxxi\n",
      "after removing urls: i'm nike soho - @nikenyc new york \n",
      "after removing HTMLs: i'm nike soho - @nikenyc new york \n",
      "after tokenize: ['i', \"'m\", 'nike', 'soho', '-', '@', 'nikenyc', 'new', 'york']\n",
      "after lemma: i 'm nike soho - @ nikenyc new york\n",
      "after changing emojis: i 'm nike soho - @ nikenyc new york\n",
      "after removing mentions: i m nike soho nikenyc new york\n",
      "after removing stop words: NEW KICKS üëÄ‚ô•Ô∏èüòçü´†üí∏ #Nike #AF1 #airforce1 @ Orlando, Florida https://t.co/y4qEc5fROs\n",
      "after lower case: new kicks üëÄ‚ô•Ô∏èüòçü´†üí∏ #nike #af1 #airforce1 @ orlando, florida https://t.co/y4qec5fros\n",
      "after removing urls: new kicks üëÄ‚ô•Ô∏èüòçü´†üí∏ #nike #af1 #airforce1 @ orlando, florida \n",
      "after removing HTMLs: new kicks üëÄ‚ô•Ô∏èüòçü´†üí∏ #nike #af1 #airforce1 @ orlando, florida \n",
      "after tokenize: ['new', 'kicks', 'üëÄ‚ô•Ô∏èüòç\\U0001fae0üí∏', '#', 'nike', '#', 'af1', '#', 'airforce1', '@', 'orlando', ',', 'florida']\n",
      "after lemma: new kick üëÄ‚ô•Ô∏èüòçü´†üí∏ # nike # af1 # airforce1 @ orlando , florida\n",
      "after changing emojis: new kick eyesheart_suitÔ∏èsmiling_face_with_heart-eyesü´†money_with_wings # nike # af1 # airforce1 @ orlando , florida\n",
      "after removing mentions: new kick eyesheart suit smiling face with heart eyes money with wings nike af1 airforce1 orlando florida\n",
      "after removing stop words: ‚ÄúGood Times‚Äù last night City Parks Foundation Gala I felt like I Got Lucky Nike Rodgers &amp; Chic Summerstage. @ Upper East Side https://t.co/FWOVnFJT6e\n",
      "after lower case: ‚Äúgood times‚Äù last night city parks foundation gala i felt like i got lucky nike rodgers &amp; chic summerstage. @ upper east side https://t.co/fwovnfjt6e\n",
      "after removing urls: ‚Äúgood times‚Äù last night city parks foundation gala i felt like i got lucky nike rodgers &amp; chic summerstage. @ upper east side \n",
      "after removing HTMLs: ‚Äúgood times‚Äù last night city parks foundation gala i felt like i got lucky nike rodgers & chic summerstage. @ upper east side \n",
      "after tokenize: ['‚Äú', 'good', 'times', '‚Äù', 'last', 'night', 'city', 'parks', 'foundation', 'gala', 'i', 'felt', 'like', 'i', 'got', 'lucky', 'nike', 'rodgers', '&', 'chic', 'summerstage', '.', '@', 'upper', 'east', 'side']\n",
      "after lemma: ‚Äú good time ‚Äù last night city park foundation gala i felt like i get lucky nike rodgers & chic summerstage . @ upper east side\n",
      "after changing emojis: ‚Äú good time ‚Äù last night city park foundation gala i felt like i get lucky nike rodgers & chic summerstage . @ upper east side\n",
      "after removing mentions: good time last night city park foundation gala i felt like i get lucky nike rodgers chic summerstage upper east side\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10    whatcha think these legging scrunchy fridayvib...\n",
       "11             sheesh these dunk go crazy nike justdoit\n",
       "12                       i m nike soho nikenyc new york\n",
       "13    new kick eyesheart suit smiling face with hear...\n",
       "14    good time last night city park foundation gala...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# snippet of the text preprocessing results\n",
    "test.apply(lambda test: clean_text_lemma_show(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_lemma(var):\n",
    "    \"\"\"\n",
    "    Function for text preprocessing with Lemmatizing with POS tag.\n",
    "    \"\"\"\n",
    "    # remove the stop words \n",
    "    sw = set(stopwords.words('english'))\n",
    "    my_text = [word for word in str(var).split() if word not in sw]\n",
    "    my_text = \" \".join(my_text)\n",
    "\n",
    "    # lowercase \n",
    "    my_text = my_text.lower()\n",
    "\n",
    "    # removal of URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    my_text = url_pattern.sub(r'', my_text)\n",
    "\n",
    "    # removal of HTML Tags\n",
    "    my_text = BeautifulSoup(my_text, \"lxml\").text\n",
    "\n",
    "    # tokenize the word using nltk  \n",
    "    my_text = nltk.word_tokenize(my_text)\n",
    "\n",
    "    # lemmatizing and using grouped word chuncks \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\": wordnet.NOUN,\"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    # lemmatizing \n",
    "    pos_tagged_text = nltk.pos_tag(my_text) \n",
    "    my_text = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "    \n",
    "    # change emojis into words \n",
    "    for emot in UNICODE_EMOJI:\n",
    "        my_text = my_text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "\n",
    "    # remove not english characters, lower case and split the text \n",
    "    my_text = re.sub('[^A-Za-z0-9]+', \" \", my_text).lower().strip() \n",
    "\n",
    "    # convert the text to list as the vectorized words  \n",
    "    ## my_text = my_text.split(\" \")\n",
    "\n",
    "    return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement on the whole text files \n",
    "text_clean_object = text_raw.apply(lambda text: clean_text_lemma(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11648"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_clean_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shanedaleaz totally the nike uniform since rep...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hat tom sachs marketing team nike they release...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the look makiracook face face with tears of jo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check new pickup nike snkrs</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jameslfreelance jumpman23 nike nikestore oooo ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Id\n",
       "0  shanedaleaz totally the nike uniform since rep...   0\n",
       "1  hat tom sachs marketing team nike they release...   1\n",
       "2  the look makiracook face face with tears of jo...   2\n",
       "3                        check new pickup nike snkrs   3\n",
       "4  jameslfreelance jumpman23 nike nikestore oooo ...   4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the clean dataset into dataframe \n",
    "text_clean = pd.DataFrame(text_clean_object)\n",
    "text_clean['Id'] = [num for num in range(0,len(text_clean_object))]\n",
    "text_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ShaneDaleAZ Totally. The Nike uniforms since ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hats off to Tom Sachs and the marketing team a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The look on @makiracook face! üòÇ \\n\\nThanks @Ni...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check out my new pickup from Nike‚Å† SNKRS: http...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@jameslfreelance @Jumpman23 @Nike @nikestore O...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Id\n",
       "0  @ShaneDaleAZ Totally. The Nike uniforms since ...   0\n",
       "1  Hats off to Tom Sachs and the marketing team a...   1\n",
       "2  The look on @makiracook face! üòÇ \\n\\nThanks @Ni...   2\n",
       "3  Check out my new pickup from Nike‚Å† SNKRS: http...   3\n",
       "4  @jameslfreelance @Jumpman23 @Nike @nikestore O...   4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe try to put the raw data in frist \n",
    "text_raw_df = pd.DataFrame(text_raw)\n",
    "text_raw_df['Id'] = [num for num in range(0,len(text_raw_df))]\n",
    "text_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 VADER Sentiment Scoring \n",
    "VADER (Valence Aware Dictionary and Sentiment Reasoner) : Bag of words approach \n",
    "\n",
    "*__Notes:__* <br>\n",
    "This model does not include relationship in words \n",
    "\n",
    "*__Steps:__* <br>\n",
    "1. Stop words are removed.\n",
    "2. Each word is scored and combined to a total score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk packages \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# progress bar tracker for some loops \n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.334, 'pos': 0.666, 'compound': 0.6115}\n",
      "{'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'compound': -0.6249}\n"
     ]
    }
   ],
   "source": [
    "# create the sentiment analysis object, the returning scores are from 0 to 1\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# show case of the returns, the final `compound` result is from -1 to 1 \n",
    "print(sia.polarity_scores(\"I'm so happy\"))\n",
    "print(sia.polarity_scores(\"This is the worst thing ever\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3743ff07c6ae45e480ab2717c5336963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment analysis using the cleaned text file \n",
    "res = {}\n",
    "for i, row in tqdm(text_clean.iterrows(), total = len(text_clean)):\n",
    "    text = row['text']\n",
    "    myid = row['Id']\n",
    "    res[myid] = sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>shanedaleaz totally the nike uniform since rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>hat tom sachs marketing team nike they release...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>the look makiracook face face with tears of jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>check new pickup nike snkrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>jameslfreelance jumpman23 nike nikestore oooo ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    neg    neu    pos  compound  \\\n",
       "0   0  0.000  1.000  0.000    0.0000   \n",
       "1   1  0.211  0.657  0.131   -0.0772   \n",
       "2   2  0.114  0.482  0.404    0.7003   \n",
       "3   3  0.000  1.000  0.000    0.0000   \n",
       "4   4  0.083  0.623  0.294    0.7269   \n",
       "\n",
       "                                                text  \n",
       "0  shanedaleaz totally the nike uniform since rep...  \n",
       "1  hat tom sachs marketing team nike they release...  \n",
       "2  the look makiracook face face with tears of jo...  \n",
       "3                        check new pickup nike snkrs  \n",
       "4  jameslfreelance jumpman23 nike nikestore oooo ...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the sentiment score back to the dataframe\n",
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns={'index':'Id'})\n",
    "vaders = vaders.merge(text_clean, how = 'left')\n",
    "\n",
    "# now we have sentiment score and metadata\n",
    "vaders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.7184</td>\n",
       "      <td>need tht nike ski mask shit look fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>williamfeltner9 these come local shop pair nike hasn t ship yet either</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.6705</td>\n",
       "      <td>ethalorian nike yeah fine you see stock tech investment look like high last year lol recession hit everyone point</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    compound  \\\n",
       "50   -0.7184   \n",
       "51    0.0000   \n",
       "52    0.6705   \n",
       "\n",
       "                                                                                                                 text  \n",
       "50                                                                              need tht nike ski mask shit look fire  \n",
       "51                                             williamfeltner9 these come local shop pair nike hasn t ship yet either  \n",
       "52  ethalorian nike yeah fine you see stock tech investment look like high last year lol recession hit everyone point  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a snippet of how it performs \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "vaders[['compound','text']][50:53]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Analysis*__\n",
    "\n",
    "The above three lines of test texts shows that since VADER doesn't take the relationship between words into consideration, in the 50th line that it made the wrong judgement of the sentiment text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Learning Models with Hugging Face\n",
    "[Reference from HuggingFace](https://huggingface.co/blog/sentiment-analysis-python) <br>\n",
    "HuggingFace: A hub that provides the collection of pre-trained models, and some are state of art. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 twitter-roberta-base-sentiment <br>\n",
    "\n",
    "[Reference for NLTK's VADER and Hugging Face Transformers using Roberta](https://www.youtube.com/watch?v=QpzMWQvxXWk)\n",
    "\n",
    "- Bert : Transformer based deep learning models\n",
    "- [roBERTa-based model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment?text=I+like+you.+I+love+you): trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hugging face library Transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pre-trained models with the Roberta \n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run example for VADER moel \n",
    "example = 'i need tht nike ski mask shit looks fire'\n",
    "print(sia.polarity_scores(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roberta_neg': 0.013050234, 'roberta_neu': 0.13767199, 'roberta_pos': 0.8492778}\n"
     ]
    }
   ],
   "source": [
    "# run example on roBERTa model \n",
    "\n",
    "# return the 0 or 1 tensors that embeding models will understand\n",
    "encoded_text = tokenizer(example, return_tensors = 'pt') # pt for PyTorch\n",
    "# the output is a tensor \n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "# apply softmax to turn into 0 to 1 range \n",
    "scores = softmax(scores)\n",
    "scores_dict = {'roberta_neg': scores[0],\n",
    "                'roberta_neu': scores[1],\n",
    "                'roberta_pos': scores[2]}\n",
    "\n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Analysis*__\n",
    "\n",
    "Yay! roBERTa correctly classified the text of \n",
    "> i need tht nike ski mask shit looks fire \n",
    "\n",
    "as being positive, much more powerful when taking sementaic meaning into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roerta( var ):\n",
    "    encoded_text = tokenizer(var, return_tensors = 'pt') # pt for PyTorch\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {'roberta_neg': scores[0],\n",
    "                    'roberta_neu': scores[1],\n",
    "                    'roberta_pos': scores[2]}\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37302c23e554bdd98961a6686111987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = {}\n",
    "for i, row in tqdm(text_clean.iterrows(), total = len(text_raw_df)):\n",
    "    try: \n",
    "        text = row['text']\n",
    "        myid = row['Id']\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "                vader_result_rename[f\"vader_{key}\"] = value\n",
    "        roberta_result = polarity_scores_roerta(text)\n",
    "        both = {**vader_result_rename, **roberta_result}\n",
    "        res[myid] = both \n",
    "    except RuntimeError: \n",
    "        print(f'Broke for id{myid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(res).T\n",
    "results_df = results_df.reset_index().rename(columns={'index':'Id'})\n",
    "results_df = results_df.merge(text_raw_df, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>0.537855</td>\n",
       "      <td>0.432637</td>\n",
       "      <td>@ShaneDaleAZ Totally. The Nike uniforms since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>0.071209</td>\n",
       "      <td>0.369753</td>\n",
       "      <td>0.559038</td>\n",
       "      <td>Hats off to Tom Sachs and the marketing team a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.077772</td>\n",
       "      <td>0.919799</td>\n",
       "      <td>The look on @makiracook face! üòÇ \\n\\nThanks @Ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.864787</td>\n",
       "      <td>0.093497</td>\n",
       "      <td>Check out my new pickup from Nike‚Å† SNKRS: http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.033002</td>\n",
       "      <td>0.965156</td>\n",
       "      <td>@jameslfreelance @Jumpman23 @Nike @nikestore O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  vader_neg  vader_neu  vader_pos  vader_compound  roberta_neg  \\\n",
       "0   0      0.000      1.000      0.000          0.0000     0.029508   \n",
       "1   1      0.211      0.657      0.131         -0.0772     0.071209   \n",
       "2   2      0.114      0.482      0.404          0.7003     0.002428   \n",
       "3   3      0.000      1.000      0.000          0.0000     0.041717   \n",
       "4   4      0.083      0.623      0.294          0.7269     0.001843   \n",
       "\n",
       "   roberta_neu  roberta_pos                                               text  \n",
       "0     0.537855     0.432637  @ShaneDaleAZ Totally. The Nike uniforms since ...  \n",
       "1     0.369753     0.559038  Hats off to Tom Sachs and the marketing team a...  \n",
       "2     0.077772     0.919799  The look on @makiracook face! üòÇ \\n\\nThanks @Ni...  \n",
       "3     0.864787     0.093497  Check out my new pickup from Nike‚Å† SNKRS: http...  \n",
       "4     0.033002     0.965156  @jameslfreelance @Jumpman23 @Nike @nikestore O...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 bertweet-base-sentiment-analysis <br> \n",
    "- [Model](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis?text=I+like+you.+I+love+you) trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on _**English tweets**_.\n",
    "\n",
    "- Uses POS, NEG, NEU labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "# use pre-trained models with the Roberta in English \n",
    "MODEL = f\"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roberta_neg': 0.00656319, 'roberta_neu': 0.17408508, 'roberta_pos': 0.8193517}\n"
     ]
    }
   ],
   "source": [
    "# run example on RoBERTa model \n",
    "example = 'i need tht nike ski mask shit looks fire'\n",
    "\n",
    "# return the 0 or 1 tensors that embeding models will understand\n",
    "encoded_text = tokenizer(example, return_tensors = 'pt') # pt for PyTorch\n",
    "# the output is a tensor \n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "# apply softmax to turn into 0 to 1 range \n",
    "scores = softmax(scores)\n",
    "scores_dict = {'roberta_neg': scores[0],\n",
    "                'roberta_neu': scores[1],\n",
    "                'roberta_pos': scores[2]}\n",
    "\n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Flair \n",
    "This is the large 18-class NER(named entity recognition) model for English that ships with Flair.\n",
    "[Reference](https://huggingface.co/flair/ner-english-ontonotes-large?text=On+September+1st+George+won+1+dollar+while+watching+Game+of+Thrones.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef4c44cf36b48a7b1c1ef7d6556c4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:13:57,871 loading file C:\\Users\\hs324\\.flair\\models\\ner-english-ontonotes-large\\2da6c2cdd76e59113033adf670340bfd820f0301ae2e39204d67ba2dc276cc28.ec1bdb304b6c66111532c3b1fc6e522460ae73f1901848a4d0362cdf9760edb1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e453fe4b2ce455e9b6195e89ff62637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba3c5ed4f5246a89c23ae7621eff6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3338a82147764a438b1028156551410e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:14:12,415 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n",
      "Sentence: \"On September 1st George won 1 dollar while watching Game of Thrones .\" ‚Üí [\"September 1st\"/DATE, \"George\"/PERSON, \"1 dollar\"/MONEY, \"Game of Thrones\"/WORK_OF_ART]\n",
      "The following NER tags are found:\n",
      "Span[1:3]: \"September 1st\" ‚Üí DATE (1.0)\n",
      "Span[3:4]: \"George\" ‚Üí PERSON (1.0)\n",
      "Span[5:7]: \"1 dollar\" ‚Üí MONEY (1.0)\n",
      "Span[9:12]: \"Game of Thrones\" ‚Üí WORK_OF_ART (1.0)\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")\n",
    "\n",
    "# make example sentence\n",
    "sentence = Sentence(\"On September 1st George won 1 dollar while watching Game of Thrones.\")\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence\n",
    "print(sentence)\n",
    "\n",
    "# print predicted NER spans\n",
    "print('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Compare Model Performance on Benchmark Labeled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load benchmark labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162980, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised ‚Äúminimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  Id\n",
       "0  when modi promised ‚Äúminimum government maximum...      -1.0   0\n",
       "1  talk all the nonsense and continue all the dra...       0.0   1\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0   2\n",
       "3  asking his supporters prefix chowkidar their n...       1.0   3\n",
       "4  answer who among these the most powerful world...       1.0   4"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import labeled dataset to test on \n",
    "labeled_df = pd.read_csv('c:\\\\Users\\\\hs324\\\\OneDrive\\\\Desktop\\\\Class_Files\\\\06_2022Fall\\\\04_Practicum\\\\Quantilope_Core\\\\data\\\\twitter_labeled_data.csv') \n",
    "labeled_df['Id'] = [num for num in range(0,len(labeled_df))]\n",
    "labeled_df.rename(columns={'clean_text':'text'},inplace = True)\n",
    "\n",
    "# have a look at the labeled dataframe\n",
    "print(labeled_df.shape)\n",
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modi promise minimum government maximum govern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say vote modi welcome bjp tell rahul main camp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ask supporter prefix chowkidar name modi great...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Id\n",
       "0  modi promise minimum government maximum govern...   0\n",
       "1             talk nonsense continue drama vote modi   1\n",
       "2  say vote modi welcome bjp tell rahul main camp...   2\n",
       "3  ask supporter prefix chowkidar name modi great...   3\n",
       "4  answer among powerful world leader today trump...   4"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement on the whole text files \n",
    "labeled_text_clean_object = labeled_df['text'].apply(lambda text: clean_text_lemma(text)) \n",
    "\n",
    "# convert the clean dataset into dataframe \n",
    "labeled_text_clean = pd.DataFrame(labeled_text_clean_object)\n",
    "labeled_text_clean['Id'] = [num for num in range(0,len(labeled_text_clean))]\n",
    "labeled_text_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove infinite and null values \n",
    "def remove_inf(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modi promise minimum government maximum govern...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say vote modi welcome bjp tell rahul main camp...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ask supporter prefix chowkidar name modi great...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Id  category\n",
       "0  modi promise minimum government maximum govern...   0      -1.0\n",
       "1             talk nonsense continue drama vote modi   1       0.0\n",
       "2  say vote modi welcome bjp tell rahul main camp...   2       1.0\n",
       "3  ask supporter prefix chowkidar name modi great...   3       1.0\n",
       "4  answer among powerful world leader today trump...   4       1.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the cleaned dataset with the labeled sentiment score\n",
    "labeled_clean_score =  labeled_text_clean.merge(labeled_df,how = 'left',on = 'Id')\\\n",
    "    .drop(columns={'text_y'}).\\\n",
    "        rename(columns={'text_x':'text'})\n",
    "labeled_clean_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building Up Baseline Model\n",
    "I'll use NLTK's VADER as the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Libraries Imported!\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, auc, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('‚úîÔ∏è Libraries Imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [117], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# split the dataset into training and testing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m y\u001b[39m=\u001b[39mlabeled_clean_score[\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m train_df, test_df \u001b[39m=\u001b[39m train_test_split(labeled_clean_score, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m, stratify \u001b[39m=\u001b[39;49m y)\n\u001b[0;32m      5\u001b[0m \u001b[39m# split the train and set into X_train and y_train sets\u001b[39;00m\n\u001b[0;32m      6\u001b[0m X_train \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hs324\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2197\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2191\u001b[0m         CVClass \u001b[39m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2193\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test,\n\u001b[0;32m   2194\u001b[0m                  train_size\u001b[39m=\u001b[39mn_train,\n\u001b[0;32m   2195\u001b[0m                  random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2197\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39;49msplit(X\u001b[39m=\u001b[39;49marrays[\u001b[39m0\u001b[39;49m], y\u001b[39m=\u001b[39;49mstratify))\n\u001b[0;32m   2199\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(chain\u001b[39m.\u001b[39mfrom_iterable((_safe_indexing(a, train),\n\u001b[0;32m   2200\u001b[0m                                  _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays))\n",
      "File \u001b[1;32mc:\\Users\\hs324\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1793\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1759\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1760\u001b[0m     \u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \n\u001b[0;32m   1762\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[39m    to an integer.\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1793\u001b[0m     y \u001b[39m=\u001b[39m check_array(y, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1794\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[1;32mc:\\Users\\hs324\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[0;32m     67\u001b[0m             \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[0;32m     68\u001b[0m                                  args[\u001b[39m-\u001b[39mextra_args:])]\n",
      "File \u001b[1;32mc:\\Users\\hs324\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:720\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    717\u001b[0m                          \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name))\n\u001b[0;32m    719\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 720\u001b[0m         _assert_all_finite(array,\n\u001b[0;32m    721\u001b[0m                            allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    723\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    724\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\hs324\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:103\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m (allow_nan \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m\n\u001b[0;32m    101\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()):\n\u001b[0;32m    102\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 103\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m                 msg_err\u001b[39m.\u001b[39mformat\n\u001b[0;32m    105\u001b[0m                 (type_err,\n\u001b[0;32m    106\u001b[0m                  msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    107\u001b[0m         )\n\u001b[0;32m    108\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# split the dataset into training and testing\n",
    "y=labeled_clean_score['category']\n",
    "train_df, test_df = train_test_split(labeled_clean_score, test_size=0.2, random_state=42, stratify = y)\n",
    "\n",
    "# split the train and set into X_train and y_train sets\n",
    "X_train = train_df.drop(columns='category')\n",
    "y_train = train_df['category']\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f'Train set: {train_df.shape[0]} rows x {train_df.shape[1]} columns')\n",
    "print(f'Test set: {test_df.shape[0]} rows x {test_df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124424</th>\n",
       "      <td>asia politics indian modi visit hugged whole w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112795</th>\n",
       "      <td>luxury car own nirav modi auction economic time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>legitimate arrest antony make nirav modi vijay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27583</th>\n",
       "      <td>respect sir sar madam kindly vaddi successfull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45216</th>\n",
       "      <td>envision india think act two step ahead curren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138908</th>\n",
       "      <td>shawl bjp symbol lotus batch like modi time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119194</th>\n",
       "      <td>thing do modi bhakts would waah modi waah mast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86731</th>\n",
       "      <td>complete analysis pure facts employment growth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64318</th>\n",
       "      <td>thats india need modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44295</th>\n",
       "      <td>india enter super league elite space power ope...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130378 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "124424  asia politics indian modi visit hugged whole w...\n",
       "112795    luxury car own nirav modi auction economic time\n",
       "39997   legitimate arrest antony make nirav modi vijay...\n",
       "27583   respect sir sar madam kindly vaddi successfull...\n",
       "45216   envision india think act two step ahead curren...\n",
       "...                                                   ...\n",
       "138908        shawl bjp symbol lotus batch like modi time\n",
       "119194  thing do modi bhakts would waah modi waah mast...\n",
       "86731   complete analysis pure facts employment growth...\n",
       "64318                               thats india need modi\n",
       "44295   india enter super league elite space power ope...\n",
       "\n",
       "[130378 rows x 1 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9141b1cae0a846a9ac70f5e4df2b6e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [87], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m tqdm(pd\u001b[39m.\u001b[39mDataFrame(X_train)\u001b[39m.\u001b[39miterrows(), total \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_train)):\n\u001b[0;32m      4\u001b[0m     text \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m     myid \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39;49m\u001b[39mId\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      6\u001b[0m     res_labeled[myid] \u001b[39m=\u001b[39m sia\u001b[39m.\u001b[39mpolarity_scores(text)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\series.py:942\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    941\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 942\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    944\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    945\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    946\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    947\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\series.py:1051\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1050\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1052\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n\u001b[0;32m   3366\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Id'"
     ]
    }
   ],
   "source": [
    "# run the vader model over training set \n",
    "res = {}\n",
    "for i, row in tqdm(X_train.iterrows(), total = len(X_train)):\n",
    "    text = row['text']\n",
    "    myid = row['Id']\n",
    "    res[myid] = sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11647: {'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.5994}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11647</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  compound  neg    neu    pos\n",
       "0  11647    0.5994  0.0  0.698  0.302"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the sentiment score back to the dataframe\n",
    "vader_labeled = pd.DataFrame(res).T\n",
    "vader_labeled = vader_labeled.reset_index().rename(columns={'index':'Id'})\n",
    "# vader_labeled = vaders.merge(text_clean, how = 'left')\n",
    "\n",
    "# now we have sentiment score and metadata\n",
    "vader_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note:**_ <br>\n",
    "Currently run into some errors and need more work on the performance comparision and building evaluation matrixs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Export Sentiment Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before coming up with the evaluation for different model performance, I'll use RoBerta model and its output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029508</td>\n",
       "      <td>0.537855</td>\n",
       "      <td>0.432637</td>\n",
       "      <td>@ShaneDaleAZ Totally. The Nike uniforms since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.071209</td>\n",
       "      <td>0.369753</td>\n",
       "      <td>0.559038</td>\n",
       "      <td>Hats off to Tom Sachs and the marketing team a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.077772</td>\n",
       "      <td>0.919799</td>\n",
       "      <td>The look on @makiracook face! üòÇ \\n\\nThanks @Ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.864787</td>\n",
       "      <td>0.093497</td>\n",
       "      <td>Check out my new pickup from Nike‚Å† SNKRS: http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.033002</td>\n",
       "      <td>0.965156</td>\n",
       "      <td>@jameslfreelance @Jumpman23 @Nike @nikestore O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11643</th>\n",
       "      <td>0.283802</td>\n",
       "      <td>0.640393</td>\n",
       "      <td>0.075805</td>\n",
       "      <td>Armpit musty, Reebok crusty https://t.co/KjJ3z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11644</th>\n",
       "      <td>0.018777</td>\n",
       "      <td>0.809956</td>\n",
       "      <td>0.171267</td>\n",
       "      <td>@DoubleOhNegatve I believe that was the Reebok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11645</th>\n",
       "      <td>0.253581</td>\n",
       "      <td>0.710993</td>\n",
       "      <td>0.035426</td>\n",
       "      <td>If you ever find yourself talking shit on will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11646</th>\n",
       "      <td>0.046945</td>\n",
       "      <td>0.874524</td>\n",
       "      <td>0.078531</td>\n",
       "      <td>Think the 4,5 shoes gon be Reebok, Doc‚Äôs and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11647</th>\n",
       "      <td>0.086358</td>\n",
       "      <td>0.759205</td>\n",
       "      <td>0.154437</td>\n",
       "      <td>üî• It is back to school shopping time and what ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11648 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       roberta_neg  roberta_neu  roberta_pos  \\\n",
       "0         0.029508     0.537855     0.432637   \n",
       "1         0.071209     0.369753     0.559038   \n",
       "2         0.002428     0.077772     0.919799   \n",
       "3         0.041717     0.864787     0.093497   \n",
       "4         0.001843     0.033002     0.965156   \n",
       "...            ...          ...          ...   \n",
       "11643     0.283802     0.640393     0.075805   \n",
       "11644     0.018777     0.809956     0.171267   \n",
       "11645     0.253581     0.710993     0.035426   \n",
       "11646     0.046945     0.874524     0.078531   \n",
       "11647     0.086358     0.759205     0.154437   \n",
       "\n",
       "                                                    text  \n",
       "0      @ShaneDaleAZ Totally. The Nike uniforms since ...  \n",
       "1      Hats off to Tom Sachs and the marketing team a...  \n",
       "2      The look on @makiracook face! üòÇ \\n\\nThanks @Ni...  \n",
       "3      Check out my new pickup from Nike‚Å† SNKRS: http...  \n",
       "4      @jameslfreelance @Jumpman23 @Nike @nikestore O...  \n",
       "...                                                  ...  \n",
       "11643  Armpit musty, Reebok crusty https://t.co/KjJ3z...  \n",
       "11644  @DoubleOhNegatve I believe that was the Reebok...  \n",
       "11645  If you ever find yourself talking shit on will...  \n",
       "11646  Think the 4,5 shoes gon be Reebok, Doc‚Äôs and s...  \n",
       "11647  üî• It is back to school shopping time and what ...  \n",
       "\n",
       "[11648 rows x 4 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_score = results_df[['roberta_neg','roberta_neu','roberta_pos','text']]\n",
    "sentiment_score.to_csv('c:\\\\Users\\\\hs324\\\\OneDrive\\\\Desktop\\\\Class_Files\\\\06_2022Fall\\\\04_Practicum\\\\Quantilope_Core\\\\data\\\\sentiment_score_text.csv')\n",
    "sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22f19ce86b3867e446285b3f1a72227a98aa5a90743b2ef78c9d35e67c033ac6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
