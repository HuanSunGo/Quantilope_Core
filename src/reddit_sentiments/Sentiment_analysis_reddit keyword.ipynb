{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99414be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Libraries Imported!\n"
     ]
    }
   ],
   "source": [
    "# for text processing and cleaning \n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string \n",
    "string.punctuation \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# remove warnings \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# for sentiment analysis \n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# for topic modeling using LDA \n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel \n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('‚úîÔ∏è Libraries Imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dee778",
   "metadata": {},
   "source": [
    "## 1. NLTK with Vader\n",
    "[NLTK Vader Documentation](https://www.nltk.org/howto/sentiment.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf029221",
   "metadata": {},
   "source": [
    "### 1.1 Understand the Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362e290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import nltk \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7eb1db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/apple/Desktop/Quantilope/Reddit_api data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6796b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Post URL</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Brought to you by Nike . Oh wait</td>\n",
       "      <td>Nbamemes</td>\n",
       "      <td>ymumjy</td>\n",
       "      <td>https://i.redd.it/kx6pa20g15y91.jpg</td>\n",
       "      <td>Kanye Irving ladies and gentlemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Brought to you by Nike . Oh wait</td>\n",
       "      <td>Nbamemes</td>\n",
       "      <td>ymumjy</td>\n",
       "      <td>https://i.redd.it/kx6pa20g15y91.jpg</td>\n",
       "      <td>Black man tweets link to amazon. Never said a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Brought to you by Nike . Oh wait</td>\n",
       "      <td>Nbamemes</td>\n",
       "      <td>ymumjy</td>\n",
       "      <td>https://i.redd.it/kx6pa20g15y91.jpg</td>\n",
       "      <td>Spot the kike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Summertime Beauty [Plymouth by nike]</td>\n",
       "      <td>AzureLane</td>\n",
       "      <td>yjou5n</td>\n",
       "      <td>https://i.redd.it/ebg5800bbfx91.jpg</td>\n",
       "      <td>Plymouth is beautiful. I know Ohisashiburi mig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Summertime Beauty [Plymouth by nike]</td>\n",
       "      <td>AzureLane</td>\n",
       "      <td>yjou5n</td>\n",
       "      <td>https://i.redd.it/ebg5800bbfx91.jpg</td>\n",
       "      <td>[Sauce](https://www.pixiv.net/en/artworks/1024...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                                 Title  Subreddit  \\\n",
       "0             0         0.0      Brought to you by Nike . Oh wait   Nbamemes   \n",
       "1             1         1.0      Brought to you by Nike . Oh wait   Nbamemes   \n",
       "2             2         2.0      Brought to you by Nike . Oh wait   Nbamemes   \n",
       "3             3         3.0  Summertime Beauty [Plymouth by nike]  AzureLane   \n",
       "4             4         4.0  Summertime Beauty [Plymouth by nike]  AzureLane   \n",
       "\n",
       "  Post ID                             Post URL  \\\n",
       "0  ymumjy  https://i.redd.it/kx6pa20g15y91.jpg   \n",
       "1  ymumjy  https://i.redd.it/kx6pa20g15y91.jpg   \n",
       "2  ymumjy  https://i.redd.it/kx6pa20g15y91.jpg   \n",
       "3  yjou5n  https://i.redd.it/ebg5800bbfx91.jpg   \n",
       "4  yjou5n  https://i.redd.it/ebg5800bbfx91.jpg   \n",
       "\n",
       "                                            Comments  \n",
       "0                  Kanye Irving ladies and gentlemen  \n",
       "1  Black man tweets link to amazon. Never said a ...  \n",
       "2                                      Spot the kike  \n",
       "3  Plymouth is beautiful. I know Ohisashiburi mig...  \n",
       "4  [Sauce](https://www.pixiv.net/en/artworks/1024...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edab0605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    Kanye Irving ladies and gentlemen\n",
       "1    Black man tweets link to amazon. Never said a ...\n",
       "2                                        Spot the kike\n",
       "3    Plymouth is beautiful. I know Ohisashiburi mig...\n",
       "4    [Sauce](https://www.pixiv.net/en/artworks/1024...\n",
       "Name: Comments, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_raw = df['Comments']\n",
    "text_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778b30f",
   "metadata": {},
   "source": [
    "### 1.2 Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df3ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb1adc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hilarious face_with_tears_of_joy. The feeling of making a sale smiling_face_with_sunglasses, The feeling of actually fulfilling orders unamused_face'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for converting emojis into word\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "text1 = \"Hilarious üòÇ. The feeling of making a sale üòé, The feeling of actually fulfilling orders üòí\"\n",
    "convert_emojis(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3fdc264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Happy_face_smiley Happy_face_smiley'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for converting emoticons into word\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = text.replace(emot, EMOTICONS_EMO[emot].replace(\" \",\"_\"))\n",
    "    return text\n",
    "\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c92527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_lemma_show(var):\n",
    "    \"\"\"\n",
    "    Function for text preprocessing with Lemmatizing with POS tag.\n",
    "    \"\"\"\n",
    "    # remove the stop words \n",
    "    sw = set(stopwords.words('english'))\n",
    "    my_text = [word for word in str(var).split() if word not in sw]\n",
    "    my_text = \" \".join(my_text)\n",
    "    print(f'after removing stop words: {my_text}')\n",
    "\n",
    "    # lowercase \n",
    "    my_text = my_text.lower()\n",
    "    print(f'after lower case: {my_text}')\n",
    "\n",
    "    # removal of URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    my_text = url_pattern.sub(r'', my_text)\n",
    "    print(f'after removing urls: {my_text}')\n",
    "\n",
    "    # removal of HTML Tags\n",
    "    my_text = BeautifulSoup(my_text, \"lxml\").text\n",
    "    print(f'after removing HTMLs: {my_text}')\n",
    "\n",
    "    # tokenize the word using nltk  \n",
    "    my_text = nltk.word_tokenize(my_text)\n",
    "    print(f'after tokenize: {my_text}')\n",
    "\n",
    "    # lemmatizing and using grouped word chuncks \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\": wordnet.NOUN,\"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    # lemmatizing \n",
    "    pos_tagged_text = nltk.pos_tag(my_text) \n",
    "    my_text = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "    print(f'after lemma: {my_text}')\n",
    "    \n",
    "    # change emojis into words \n",
    "    for emot in UNICODE_EMOJI:\n",
    "        my_text = my_text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    print(f'after changing emojis: {my_text}')\n",
    "\n",
    "    # remove not english characters, lower case and split the text \n",
    "    my_text = re.sub('[^A-Za-z0-9]+', \" \", my_text).lower().strip() \n",
    "    print(f'after removing mentions: {my_text}')\n",
    "\n",
    "    # convert the text to list as the vectorized words  \n",
    "    ## my_text = my_text.split(\" \")\n",
    "\n",
    "    return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0192579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    we are like manjuus. they might work hard righ...\n",
       "11                                 Plymouth; Just do it\n",
       "12                                            [deleted]\n",
       "13    I want this image to be the last image I see b...\n",
       "14    Just a heads up: this post has been locked, as...\n",
       "Name: Comments, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = text_raw[10:15]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96044679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing stop words: like manjuus. might work hard right, enjoy, right now. manjuu cultures, I say\n",
      "after lower case: like manjuus. might work hard right, enjoy, right now. manjuu cultures, i say\n",
      "after removing urls: like manjuus. might work hard right, enjoy, right now. manjuu cultures, i say\n",
      "after removing HTMLs: like manjuus. might work hard right, enjoy, right now. manjuu cultures, i say\n",
      "after tokenize: ['like', 'manjuus', '.', 'might', 'work', 'hard', 'right', ',', 'enjoy', ',', 'right', 'now', '.', 'manjuu', 'cultures', ',', 'i', 'say']\n",
      "after lemma: like manjuus . might work hard right , enjoy , right now . manjuu culture , i say\n",
      "after changing emojis: like manjuus . might work hard right , enjoy , right now . manjuu culture , i say\n",
      "after removing mentions: like manjuus might work hard right enjoy right now manjuu culture i say\n",
      "after removing stop words: Plymouth; Just\n",
      "after lower case: plymouth; just\n",
      "after removing urls: plymouth; just\n",
      "after removing HTMLs: plymouth; just\n",
      "after tokenize: ['plymouth', ';', 'just']\n",
      "after lemma: plymouth ; just\n",
      "after changing emojis: plymouth ; just\n",
      "after removing mentions: plymouth just\n",
      "after removing stop words: [deleted]\n",
      "after lower case: [deleted]\n",
      "after removing urls: [deleted]\n",
      "after removing HTMLs: [deleted]\n",
      "after tokenize: ['[', 'deleted', ']']\n",
      "after lemma: [ delete ]\n",
      "after changing emojis: [ delete ]\n",
      "after removing mentions: delete\n",
      "after removing stop words: I want image last image I see I die, I imagine it. I'm room, looking old disheveled, room lit police lights wail sirens (the kinds go wee-oo we-oo) Across desktop, I go reddit look image, man bullhorn yelling **\"POLICE! COME OUT AND DRINK YOUR CORN SYRUP!\"**, But I listen, instead I pull Image see Plymouth, So Serene, So Elegant, So Happy. I smile faintly I look eyes hold pistol temple. tear swells cheek, I assume know happens next. Anyways cool art\n",
      "after lower case: i want image last image i see i die, i imagine it. i'm room, looking old disheveled, room lit police lights wail sirens (the kinds go wee-oo we-oo) across desktop, i go reddit look image, man bullhorn yelling **\"police! come out and drink your corn syrup!\"**, but i listen, instead i pull image see plymouth, so serene, so elegant, so happy. i smile faintly i look eyes hold pistol temple. tear swells cheek, i assume know happens next. anyways cool art\n",
      "after removing urls: i want image last image i see i die, i imagine it. i'm room, looking old disheveled, room lit police lights wail sirens (the kinds go wee-oo we-oo) across desktop, i go reddit look image, man bullhorn yelling **\"police! come out and drink your corn syrup!\"**, but i listen, instead i pull image see plymouth, so serene, so elegant, so happy. i smile faintly i look eyes hold pistol temple. tear swells cheek, i assume know happens next. anyways cool art\n",
      "after removing HTMLs: i want image last image i see i die, i imagine it. i'm room, looking old disheveled, room lit police lights wail sirens (the kinds go wee-oo we-oo) across desktop, i go reddit look image, man bullhorn yelling **\"police! come out and drink your corn syrup!\"**, but i listen, instead i pull image see plymouth, so serene, so elegant, so happy. i smile faintly i look eyes hold pistol temple. tear swells cheek, i assume know happens next. anyways cool art\n",
      "after tokenize: ['i', 'want', 'image', 'last', 'image', 'i', 'see', 'i', 'die', ',', 'i', 'imagine', 'it', '.', 'i', \"'m\", 'room', ',', 'looking', 'old', 'disheveled', ',', 'room', 'lit', 'police', 'lights', 'wail', 'sirens', '(', 'the', 'kinds', 'go', 'wee-oo', 'we-oo', ')', 'across', 'desktop', ',', 'i', 'go', 'reddit', 'look', 'image', ',', 'man', 'bullhorn', 'yelling', '*', '*', \"''\", 'police', '!', 'come', 'out', 'and', 'drink', 'your', 'corn', 'syrup', '!', '``', '*', '*', ',', 'but', 'i', 'listen', ',', 'instead', 'i', 'pull', 'image', 'see', 'plymouth', ',', 'so', 'serene', ',', 'so', 'elegant', ',', 'so', 'happy', '.', 'i', 'smile', 'faintly', 'i', 'look', 'eyes', 'hold', 'pistol', 'temple', '.', 'tear', 'swells', 'cheek', ',', 'i', 'assume', 'know', 'happens', 'next', '.', 'anyways', 'cool', 'art']\n",
      "after lemma: i want image last image i see i die , i imagine it . i 'm room , look old dishevel , room lit police light wail siren ( the kind go wee-oo we-oo ) across desktop , i go reddit look image , man bullhorn yell * * '' police ! come out and drink your corn syrup ! `` * * , but i listen , instead i pull image see plymouth , so serene , so elegant , so happy . i smile faintly i look eye hold pistol temple . tear swell cheek , i assume know happen next . anyways cool art\n",
      "after changing emojis: i want image last image i see i die , i imagine it . i 'm room , look old dishevel , room lit police light wail siren ( the kind go wee-oo we-oo ) across desktop , i go reddit look image , man bullhorn yell * * '' police ! come out and drink your corn syrup ! `` * * , but i listen , instead i pull image see plymouth , so serene , so elegant , so happy . i smile faintly i look eye hold pistol temple . tear swell cheek , i assume know happen next . anyways cool art\n",
      "after removing mentions: i want image last image i see i die i imagine it i m room look old dishevel room lit police light wail siren the kind go wee oo we oo across desktop i go reddit look image man bullhorn yell police come out and drink your corn syrup but i listen instead i pull image see plymouth so serene so elegant so happy i smile faintly i look eye hold pistol temple tear swell cheek i assume know happen next anyways cool art\n",
      "after removing stop words: Just heads up: post locked, posts r/dirtyr4r are. This personal ads discussion topics, r/dirtyr4r discussion forum. READ THAT AGAIN. This busy subreddit - modmail us ask post locked block week+. Responses PMed OP directly. Here's convenient link: https://www.reddit.com/message/compose/?to=OpenSlide6 . If poster breaking rules, please use report button AND message moderators. *I bot, action performed automatically. Please [contact moderators subreddit](/message/compose/?to=/r/dirtyr4r) questions concerns.*\n",
      "after lower case: just heads up: post locked, posts r/dirtyr4r are. this personal ads discussion topics, r/dirtyr4r discussion forum. read that again. this busy subreddit - modmail us ask post locked block week+. responses pmed op directly. here's convenient link: https://www.reddit.com/message/compose/?to=openslide6 . if poster breaking rules, please use report button and message moderators. *i bot, action performed automatically. please [contact moderators subreddit](/message/compose/?to=/r/dirtyr4r) questions concerns.*\n",
      "after removing urls: just heads up: post locked, posts r/dirtyr4r are. this personal ads discussion topics, r/dirtyr4r discussion forum. read that again. this busy subreddit - modmail us ask post locked block week+. responses pmed op directly. here's convenient link:  . if poster breaking rules, please use report button and message moderators. *i bot, action performed automatically. please [contact moderators subreddit](/message/compose/?to=/r/dirtyr4r) questions concerns.*\n",
      "after removing HTMLs: just heads up: post locked, posts r/dirtyr4r are. this personal ads discussion topics, r/dirtyr4r discussion forum. read that again. this busy subreddit - modmail us ask post locked block week+. responses pmed op directly. here's convenient link:  . if poster breaking rules, please use report button and message moderators. *i bot, action performed automatically. please [contact moderators subreddit](/message/compose/?to=/r/dirtyr4r) questions concerns.*\n",
      "after tokenize: ['just', 'heads', 'up', ':', 'post', 'locked', ',', 'posts', 'r/dirtyr4r', 'are', '.', 'this', 'personal', 'ads', 'discussion', 'topics', ',', 'r/dirtyr4r', 'discussion', 'forum', '.', 'read', 'that', 'again', '.', 'this', 'busy', 'subreddit', '-', 'modmail', 'us', 'ask', 'post', 'locked', 'block', 'week+', '.', 'responses', 'pmed', 'op', 'directly', '.', 'here', \"'s\", 'convenient', 'link', ':', '.', 'if', 'poster', 'breaking', 'rules', ',', 'please', 'use', 'report', 'button', 'and', 'message', 'moderators', '.', '*', 'i', 'bot', ',', 'action', 'performed', 'automatically', '.', 'please', '[', 'contact', 'moderators', 'subreddit', ']', '(', '/message/compose/', '?', 'to=/r/dirtyr4r', ')', 'questions', 'concerns', '.', '*']\n",
      "after lemma: just head up : post lock , post r/dirtyr4r be . this personal ad discussion topic , r/dirtyr4r discussion forum . read that again . this busy subreddit - modmail u ask post lock block week+ . response pmed op directly . here 's convenient link : . if poster breaking rule , please use report button and message moderator . * i bot , action perform automatically . please [ contact moderator subreddit ] ( /message/compose/ ? to=/r/dirtyr4r ) question concern . *\n",
      "after changing emojis: just head up : post lock , post r/dirtyr4r be . this personal ad discussion topic , r/dirtyr4r discussion forum . read that again . this busy subreddit - modmail u ask post lock block week+ . response pmed op directly . here 's convenient link : . if poster breaking rule , please use report button and message moderator . * i bot , action perform automatically . please [ contact moderator subreddit ] ( /message/compose/ ? to=/r/dirtyr4r ) question concern . *\n",
      "after removing mentions: just head up post lock post r dirtyr4r be this personal ad discussion topic r dirtyr4r discussion forum read that again this busy subreddit modmail u ask post lock block week response pmed op directly here s convenient link if poster breaking rule please use report button and message moderator i bot action perform automatically please contact moderator subreddit message compose to r dirtyr4r question concern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10    like manjuus might work hard right enjoy right...\n",
       "11                                        plymouth just\n",
       "12                                               delete\n",
       "13    i want image last image i see i die i imagine ...\n",
       "14    just head up post lock post r dirtyr4r be this...\n",
       "Name: Comments, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# snippet of the text preprocessing results\n",
    "test.apply(lambda test: clean_text_lemma_show(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ac0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_lemma(var):\n",
    "    \"\"\"\n",
    "    Function for text preprocessing with Lemmatizing with POS tag.\n",
    "    \"\"\"\n",
    "    # remove the stop words \n",
    "    sw = set(stopwords.words('english'))\n",
    "    my_text = [word for word in str(var).split() if word not in sw]\n",
    "    my_text = \" \".join(my_text)\n",
    "\n",
    "    # lowercase \n",
    "    my_text = my_text.lower()\n",
    "\n",
    "    # removal of URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    my_text = url_pattern.sub(r'', my_text)\n",
    "\n",
    "    # removal of HTML Tags\n",
    "    my_text = BeautifulSoup(my_text, \"lxml\").text\n",
    "\n",
    "    # tokenize the word using nltk  \n",
    "    my_text = nltk.word_tokenize(my_text)\n",
    "\n",
    "    # lemmatizing and using grouped word chuncks \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\": wordnet.NOUN,\"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    # lemmatizing \n",
    "    pos_tagged_text = nltk.pos_tag(my_text) \n",
    "    my_text = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "    \n",
    "    # change emojis into words \n",
    "    for emot in UNICODE_EMOJI:\n",
    "        my_text = my_text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "\n",
    "    # remove not english characters, lower case and split the text \n",
    "    my_text = re.sub('[^A-Za-z0-9]+', \" \", my_text).lower().strip() \n",
    "\n",
    "    # convert the text to list as the vectorized words  \n",
    "    ## my_text = my_text.split(\" \")\n",
    "\n",
    "    return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07cdbd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement on the whole text files \n",
    "text_clean_object = text_raw.apply(lambda text: clean_text_lemma(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f60cd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_clean_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdfe5f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kanye irving lady gentleman</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black man tweet link amazon never say word giv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spot kike</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plymouth beautiful i know ohisashiburi might a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sauce u repostsleuthbot</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments  Id\n",
       "0                        kanye irving lady gentleman   0\n",
       "1  black man tweet link amazon never say word giv...   1\n",
       "2                                          spot kike   2\n",
       "3  plymouth beautiful i know ohisashiburi might a...   3\n",
       "4                            sauce u repostsleuthbot   4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean = pd.DataFrame(text_clean_object)\n",
    "text_clean['Id'] = [num for num in range(0,len(text_clean_object))]\n",
    "text_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a78e570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kanye Irving ladies and gentlemen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black man tweets link to amazon. Never said a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spot the kike</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Plymouth is beautiful. I know Ohisashiburi mig...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Sauce](https://www.pixiv.net/en/artworks/1024...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments  Id\n",
       "0                  Kanye Irving ladies and gentlemen   0\n",
       "1  Black man tweets link to amazon. Never said a ...   1\n",
       "2                                      Spot the kike   2\n",
       "3  Plymouth is beautiful. I know Ohisashiburi mig...   3\n",
       "4  [Sauce](https://www.pixiv.net/en/artworks/1024...   4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_raw_df = pd.DataFrame(text_raw)\n",
    "text_raw_df['Id'] = [num for num in range(0,len(text_raw_df))]\n",
    "text_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f6602",
   "metadata": {},
   "source": [
    "### 1.3 VADER Sentiment Scoring \n",
    "VADER (Valence Aware Dictionary and Sentiment Reasoner) : Bag of words approach \n",
    "\n",
    "*__Notes:__* <br>\n",
    "This model does not include relationship in words \n",
    "\n",
    "*__Steps:__* <br>\n",
    "1. Stop words are removed.\n",
    "2. Each word is scored and combined to a total score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded1efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk packages \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# progress bar tracker for some loops \n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b24f27e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.334, 'pos': 0.666, 'compound': 0.6115}\n",
      "{'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'compound': -0.6249}\n"
     ]
    }
   ],
   "source": [
    "# create the sentiment analysis object, the returning scores are from 0 to 1\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# show case of the returns, the final `compound` result is from -1 to 1 \n",
    "print(sia.polarity_scores(\"I'm so happy\"))\n",
    "print(sia.polarity_scores(\"This is the worst thing ever\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e2100c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d3346a30e84b3a829dd5deb50247e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = {}\n",
    "for i, row in tqdm(text_clean.iterrows(), total = len(text_clean)):\n",
    "    text = row['Comments']\n",
    "    myid = row['Id']\n",
    "    res[myid] = sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53c1a9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>kanye irving lady gentleman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.7906</td>\n",
       "      <td>black man tweet link amazon never say word giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>spot kike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>plymouth beautiful i know ohisashiburi might a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>sauce u repostsleuthbot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    neg    neu    pos  compound  \\\n",
       "0   0  0.000  1.000  0.000    0.0000   \n",
       "1   1  0.278  0.584  0.138   -0.7906   \n",
       "2   2  0.000  1.000  0.000    0.0000   \n",
       "3   3  0.000  0.653  0.347    0.8779   \n",
       "4   4  0.000  1.000  0.000    0.0000   \n",
       "\n",
       "                                            Comments  \n",
       "0                        kanye irving lady gentleman  \n",
       "1  black man tweet link amazon never say word giv...  \n",
       "2                                          spot kike  \n",
       "3  plymouth beautiful i know ohisashiburi might a...  \n",
       "4                            sauce u repostsleuthbot  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the sentiment score back to the dataframe\n",
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns={'index':'Id'})\n",
    "vaders = vaders.merge(text_clean, how = 'left')\n",
    "\n",
    "# now we have sentiment score and metadata\n",
    "vaders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa818c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.6908</td>\n",
       "      <td>lol good make youtube platform might nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.2516</td>\n",
       "      <td>oh goodness really obsessed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>be anyway legit check pair the toebox panel look really thick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    compound                                                       Comments\n",
       "50    0.6908                   lol good make youtube platform might nothing\n",
       "51    0.2516                                    oh goodness really obsessed\n",
       "52    0.0000  be anyway legit check pair the toebox panel look really thick"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a snippet of how it performs \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "vaders[['compound','Comments']][50:53]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76763397",
   "metadata": {},
   "source": [
    "## 2. Deep Learning Models with Hugging Face\n",
    "[Reference from HuggingFace](https://huggingface.co/blog/sentiment-analysis-python) <br>\n",
    "HuggingFace: A hub that provides the collection of pre-trained models, and some are state of art. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cb32b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hugging face library Transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589985a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ba3142452c43eca803e430d26dcbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use pre-trained models with the Roberta \n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc26d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.5, 'neu': 0.5, 'pos': 0.0, 'compound': -0.7184}\n"
     ]
    }
   ],
   "source": [
    "# run example for VADER moel \n",
    "example = 'i need tht nike ski mask shit looks fire'\n",
    "print(sia.polarity_scores(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2469126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roberta_neg': 0.013050259, 'roberta_neu': 0.13767216, 'roberta_pos': 0.8492776}\n"
     ]
    }
   ],
   "source": [
    "# run example on roBERTa model \n",
    "\n",
    "# return the 0 or 1 tensors that embeding models will understand\n",
    "encoded_text = tokenizer(example, return_tensors = 'pt') # pt for PyTorch\n",
    "# the output is a tensor \n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "# apply softmax to turn into 0 to 1 range \n",
    "scores = softmax(scores)\n",
    "scores_dict = {'roberta_neg': scores[0],\n",
    "                'roberta_neu': scores[1],\n",
    "                'roberta_pos': scores[2]}\n",
    "\n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9012ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roerta( var ):\n",
    "    encoded_text = tokenizer(var, return_tensors = 'pt') # pt for PyTorch\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {'roberta_neg': scores[0],\n",
    "                    'roberta_neu': scores[1],\n",
    "                    'roberta_pos': scores[2]}\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8013b440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08f90a73be645058d33779d76263f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broke for id57\n",
      "Broke for id454\n",
      "Broke for id462\n",
      "Broke for id665\n",
      "Broke for id760\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "for i, row in tqdm(text_clean.iterrows(), total = len(text_raw_df)):\n",
    "    try: \n",
    "        text = row['Comments']\n",
    "        myid = row['Id']\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "                vader_result_rename[f\"vader_{key}\"] = value\n",
    "        roberta_result = polarity_scores_roerta(text)\n",
    "        both = {**vader_result_rename, **roberta_result}\n",
    "        res[myid] = both \n",
    "    except RuntimeError: \n",
    "        print(f'Broke for id{myid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40abf671",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(res).T\n",
    "results_df = results_df.reset_index().rename(columns={'index':'Id'})\n",
    "results_df = results_df.merge(text_raw_df, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf30ca3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.778464</td>\n",
       "      <td>0.194813</td>\n",
       "      <td>Kanye Irving ladies and gentlemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.7906</td>\n",
       "      <td>0.546833</td>\n",
       "      <td>0.423505</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>Black man tweets link to amazon. Never said a word. Given every label in the book. \\n\\nWhite man that profits off the book and movie. No mass outrage for the white man. \\n\\nThe outrage is fake. They want another black man to say ‚Äútoby‚Äù.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.125244</td>\n",
       "      <td>0.732860</td>\n",
       "      <td>0.141896</td>\n",
       "      <td>Spot the kike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.056462</td>\n",
       "      <td>0.941035</td>\n",
       "      <td>Plymouth is beautiful. I know Ohisashiburi might not be able to use this exact design for a possible swimsuit skin, but I‚Äôd love it if they got the chance to make an official one.\\n\\nEspecially if she gets to keep the hat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.408988</td>\n",
       "      <td>0.552274</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>[Sauce](https://www.pixiv.net/en/artworks/102444636)\\n\\n&amp;#x200B;\\n\\nu/RepostSleuthBot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  vader_neg  vader_neu  vader_pos  vader_compound  roberta_neg  \\\n",
       "0   0      0.000      1.000      0.000          0.0000     0.026723   \n",
       "1   1      0.278      0.584      0.138         -0.7906     0.546833   \n",
       "2   2      0.000      1.000      0.000          0.0000     0.125244   \n",
       "3   3      0.000      0.653      0.347          0.8779     0.002503   \n",
       "4   4      0.000      1.000      0.000          0.0000     0.408988   \n",
       "\n",
       "   roberta_neu  roberta_pos  \\\n",
       "0     0.778464     0.194813   \n",
       "1     0.423505     0.029663   \n",
       "2     0.732860     0.141896   \n",
       "3     0.056462     0.941035   \n",
       "4     0.552274     0.038739   \n",
       "\n",
       "                                                                                                                                                                                                                                       Comments  \n",
       "0                                                                                                                                                                                                             Kanye Irving ladies and gentlemen  \n",
       "1  Black man tweets link to amazon. Never said a word. Given every label in the book. \\n\\nWhite man that profits off the book and movie. No mass outrage for the white man. \\n\\nThe outrage is fake. They want another black man to say ‚Äútoby‚Äù.  \n",
       "2                                                                                                                                                                                                                                 Spot the kike  \n",
       "3                Plymouth is beautiful. I know Ohisashiburi might not be able to use this exact design for a possible swimsuit skin, but I‚Äôd love it if they got the chance to make an official one.\\n\\nEspecially if she gets to keep the hat.  \n",
       "4                                                                                                                                                         [Sauce](https://www.pixiv.net/en/artworks/102444636)\\n\\n&#x200B;\\n\\nu/RepostSleuthBot  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5a14d",
   "metadata": {},
   "source": [
    "### 2.2 bertweet-base-sentiment-analysis <br> \n",
    "- [Model](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis?text=I+like+you.+I+love+you) trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on _**English tweets**_.\n",
    "\n",
    "- Uses POS, NEG, NEU labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df7bcce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81be88d614d04f559765f9d0bc966677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758648861e3740f5b315dbe24f62f2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/890 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd17f6b5a5d4e359bc2d5e7cb98e334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0949f11310aa4443adda9aa8567d899a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f1d47d751f4259a5ddcf11fd888b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c2644612cd4f849c13040c1ed1ac16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b6884a72ff43f88932d4d2531e5b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use pre-trained models with the Roberta in English \n",
    "MODEL = f\"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77da67f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roberta_neg': 0.0065631904, 'roberta_neu': 0.17408511, 'roberta_pos': 0.8193516}\n"
     ]
    }
   ],
   "source": [
    "# run example on RoBERTa model \n",
    "example = 'i need tht nike ski mask shit looks fire'\n",
    "\n",
    "# return the 0 or 1 tensors that embeding models will understand\n",
    "encoded_text = tokenizer(example, return_tensors = 'pt') # pt for PyTorch\n",
    "# the output is a tensor \n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "# apply softmax to turn into 0 to 1 range \n",
    "scores = softmax(scores)\n",
    "scores_dict = {'roberta_neg': scores[0],\n",
    "                'roberta_neu': scores[1],\n",
    "                'roberta_pos': scores[2]}\n",
    "\n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710410c",
   "metadata": {},
   "source": [
    "### 2.3 Flair \n",
    "This is the large 18-class NER(named entity recognition) model for English that ships with Flair.\n",
    "[Reference](https://huggingface.co/flair/ner-english-ontonotes-large?text=On+September+1st+George+won+1+dollar+while+watching+Game+of+Thrones.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11ef19e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3ef3f45cf04bd3a6d03833da0e900d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 22:27:58,460 loading file /Users/apple/.flair/models/ner-english-ontonotes-large/2da6c2cdd76e59113033adf670340bfd820f0301ae2e39204d67ba2dc276cc28.ec1bdb304b6c66111532c3b1fc6e522460ae73f1901848a4d0362cdf9760edb1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0058b0a55649978f6371b3730a1030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fae6b132670455092bfcc361e8acd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65fa2e4c83941e6b06f5e571beab86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 22:28:44,908 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n",
      "Sentence: \"On September 1st George won 1 dollar while watching Game of Thrones .\" ‚Üí [\"September 1st\"/DATE, \"George\"/PERSON, \"1 dollar\"/MONEY, \"Game of Thrones\"/WORK_OF_ART]\n",
      "The following NER tags are found:\n",
      "Span[1:3]: \"September 1st\" ‚Üí DATE (1.0)\n",
      "Span[3:4]: \"George\" ‚Üí PERSON (1.0)\n",
      "Span[5:7]: \"1 dollar\" ‚Üí MONEY (1.0)\n",
      "Span[9:12]: \"Game of Thrones\" ‚Üí WORK_OF_ART (1.0)\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")\n",
    "\n",
    "# make example sentence\n",
    "sentence = Sentence(\"On September 1st George won 1 dollar while watching Game of Thrones.\")\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence\n",
    "print(sentence)\n",
    "\n",
    "# print predicted NER spans\n",
    "print('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49459542",
   "metadata": {},
   "source": [
    "## 3.Compare Model Performance on Benchmark Labeled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93bae121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.778464</td>\n",
       "      <td>0.194813</td>\n",
       "      <td>Kanye Irving ladies and gentlemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.546833</td>\n",
       "      <td>0.423505</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>Black man tweets link to amazon. Never said a word. Given every label in the book. \\n\\nWhite man that profits off the book and movie. No mass outrage for the white man. \\n\\nThe outrage is fake. They want another black man to say ‚Äútoby‚Äù.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125244</td>\n",
       "      <td>0.732860</td>\n",
       "      <td>0.141896</td>\n",
       "      <td>Spot the kike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.056462</td>\n",
       "      <td>0.941035</td>\n",
       "      <td>Plymouth is beautiful. I know Ohisashiburi might not be able to use this exact design for a possible swimsuit skin, but I‚Äôd love it if they got the chance to make an official one.\\n\\nEspecially if she gets to keep the hat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.408988</td>\n",
       "      <td>0.552274</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>[Sauce](https://www.pixiv.net/en/artworks/102444636)\\n\\n&amp;#x200B;\\n\\nu/RepostSleuthBot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0.236839</td>\n",
       "      <td>0.710536</td>\n",
       "      <td>0.052625</td>\n",
       "      <td>So far only the player can use in the normal way Super tier magic. But i still ask myself if the Elf King who is half-player can not use Super tier magic or doesn't know is existance and so has never learned a Super tier spell in the first place.\\n\\nThis is just speculation from my part in the same ways that i ask myself if Ainz skill Dark Wisdom would work on a player child (especially if this is a child between 2 players).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0.047229</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.113081</td>\n",
       "      <td>The corps of the Abbys have told us that they have supposedly reach up to 6-9tier magic so I am going to go ahead n say you will need to be a long living race to accomplish super tier magic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>0.018320</td>\n",
       "      <td>0.090529</td>\n",
       "      <td>0.891151</td>\n",
       "      <td>Great story! It made my pants shrink a little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>0.122516</td>\n",
       "      <td>0.803214</td>\n",
       "      <td>0.074270</td>\n",
       "      <td>Please make a top-level comment detailing your desired roleplay scenario. Once done, reply to this comment and your submission will be approved.    \\n\\nDo not reply to this comment unless you have done as instructed. A top-level comment is a comment on this post that is **NOT** a reply to another comment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0.010849</td>\n",
       "      <td>0.292520</td>\n",
       "      <td>0.696631</td>\n",
       "      <td>I‚Äôm looking for a descriptive woman to build a small town country themed role. Your character can either be from the big city and relocated to the small town or they can be from the small town. You can be my new neighbour that just moved in or we meet at the local fair or at a party. We can discuss how our characters meet and interact with each other. I would like this to be a slice of life role with a balance of romance, drama, good story and sex. I am also okay with first or third person roleplaying, first for main characters and third for any others if we decide to play any. I would like a good amount of description and detail in this and to keep things fun and interesting.\\n\\nI would like a good amount of build up and teasing between characters in the beginning to set things up for later. Rather than just jump right into the naughty stuff.\\n\\nI would like a female character that has a bit of a naughty wild side to her and be fun. My preference are busty/ thick or curvy characters, I also prefer reference pictures as I find that much easier to grasp how your character looks. If there are any women interested in this idea please inbox me your kinks, limits and anything else you‚Äôll like to add and let‚Äôs make a fun RP.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     roberta_neg  roberta_neu  roberta_pos  \\\n",
       "0       0.026723     0.778464     0.194813   \n",
       "1       0.546833     0.423505     0.029663   \n",
       "2       0.125244     0.732860     0.141896   \n",
       "3       0.002503     0.056462     0.941035   \n",
       "4       0.408988     0.552274     0.038739   \n",
       "..           ...          ...          ...   \n",
       "876     0.236839     0.710536     0.052625   \n",
       "877     0.047229     0.839690     0.113081   \n",
       "878     0.018320     0.090529     0.891151   \n",
       "879     0.122516     0.803214     0.074270   \n",
       "880     0.010849     0.292520     0.696631   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Comments  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Kanye Irving ladies and gentlemen  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Black man tweets link to amazon. Never said a word. Given every label in the book. \\n\\nWhite man that profits off the book and movie. No mass outrage for the white man. \\n\\nThe outrage is fake. They want another black man to say ‚Äútoby‚Äù.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Spot the kike  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Plymouth is beautiful. I know Ohisashiburi might not be able to use this exact design for a possible swimsuit skin, but I‚Äôd love it if they got the chance to make an official one.\\n\\nEspecially if she gets to keep the hat.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [Sauce](https://www.pixiv.net/en/artworks/102444636)\\n\\n&#x200B;\\n\\nu/RepostSleuthBot  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...  \n",
       "876                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            So far only the player can use in the normal way Super tier magic. But i still ask myself if the Elf King who is half-player can not use Super tier magic or doesn't know is existance and so has never learned a Super tier spell in the first place.\\n\\nThis is just speculation from my part in the same ways that i ask myself if Ainz skill Dark Wisdom would work on a player child (especially if this is a child between 2 players).  \n",
       "877                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The corps of the Abbys have told us that they have supposedly reach up to 6-9tier magic so I am going to go ahead n say you will need to be a long living race to accomplish super tier magic.  \n",
       "878                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Great story! It made my pants shrink a little  \n",
       "879                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Please make a top-level comment detailing your desired roleplay scenario. Once done, reply to this comment and your submission will be approved.    \\n\\nDo not reply to this comment unless you have done as instructed. A top-level comment is a comment on this post that is **NOT** a reply to another comment.  \n",
       "880  I‚Äôm looking for a descriptive woman to build a small town country themed role. Your character can either be from the big city and relocated to the small town or they can be from the small town. You can be my new neighbour that just moved in or we meet at the local fair or at a party. We can discuss how our characters meet and interact with each other. I would like this to be a slice of life role with a balance of romance, drama, good story and sex. I am also okay with first or third person roleplaying, first for main characters and third for any others if we decide to play any. I would like a good amount of description and detail in this and to keep things fun and interesting.\\n\\nI would like a good amount of build up and teasing between characters in the beginning to set things up for later. Rather than just jump right into the naughty stuff.\\n\\nI would like a female character that has a bit of a naughty wild side to her and be fun. My preference are busty/ thick or curvy characters, I also prefer reference pictures as I find that much easier to grasp how your character looks. If there are any women interested in this idea please inbox me your kinks, limits and anything else you‚Äôll like to add and let‚Äôs make a fun RP.  \n",
       "\n",
       "[881 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_score = results_df[['roberta_neg','roberta_neu','roberta_pos','Comments']]\n",
    "sentiment_score.to_csv('c:\\\\Users\\\\hs324\\\\OneDrive\\\\Desktop\\\\Class_Files\\\\06_2022Fall\\\\04_Practicum\\\\Quantilope_Core\\\\data\\\\sentiment_score_text.csv')\n",
    "sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79e7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
